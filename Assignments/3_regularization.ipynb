{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kR-4eNdK6lYS"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 3\n",
    "------------\n",
    "\n",
    "Previously in `2_fullyconnected.ipynb`, you trained a logistic regression and a neural network model.\n",
    "\n",
    "The goal of this assignment is to explore regularization techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "JLpLa8Jt7Vu4"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1HrCK6e17WzV"
   },
   "source": [
    "First reload the data we generated in `1_notmnist.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11777,
     "status": "ok",
     "timestamp": 1449849322348,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "y3-cj1bpmuxc",
    "outputId": "e03576f1-ebbe-4838-c388-f1777bcc9873"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (100000, 28, 28) (100000,)\n",
      "Validation set (5000, 28, 28) (5000,)\n",
      "Test set (5000, 28, 28) (5000,)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  train_dataset = save['train_dataset']\n",
    "  train_labels = save['train_labels']\n",
    "  valid_dataset = save['valid_dataset']\n",
    "  valid_labels = save['valid_labels']\n",
    "  test_dataset = save['test_dataset']\n",
    "  test_labels = save['test_labels']\n",
    "  del save  # hint to help gc free up memory\n",
    "  print('Training set', train_dataset.shape, train_labels.shape)\n",
    "  print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "  print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L7aHrm6nGDMB"
   },
   "source": [
    "Reformat into a shape that's more adapted to the models we're going to train:\n",
    "- data as a flat matrix,\n",
    "- labels as float 1-hot encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11728,
     "status": "ok",
     "timestamp": 1449849322356,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "IRSyYiIIGIzS",
    "outputId": "3f8996ee-3574-4f44-c953-5c8a04636582"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (100000, 784) (100000, 10)\n",
      "Validation set (5000, 784) (5000, 10)\n",
      "Test set (5000, 784) (5000, 10)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "  dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "  # Map 1 to [0.0, 1.0, 0.0 ...], 2 to [0.0, 0.0, 1.0 ...]\n",
    "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "  return dataset, labels\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "RajPLaL_ZW6w"
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sgLbUAQ1CW-1"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "Introduce and tune L2 regularization for both logistic and neural network models. Remember that L2 amounts to adding a penalty on the norm of the weights to the loss. In TensorFlow, you can compute the L2 loss for a tensor `t` using `nn.l2_loss(t)`. The right amount of regularization should improve your validation / test accuracy.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    \n",
    "    #Input\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size*image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    beta = tf.placeholder(tf.float32)\n",
    "    \n",
    "    #Variables\n",
    "    weights = tf.Variable(tf.truncated_normal([image_size*image_size, num_labels]))\n",
    "    biases = tf.Variable(tf.zeros([num_labels]))\n",
    "    \n",
    "    #Computation\n",
    "    logits = tf.matmul(tf_train_dataset, weights) + biases\n",
    "    prbs = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=tf_train_labels)\n",
    "    loss = tf.reduce_mean(prbs + beta*tf.nn.l2_loss(weights))\n",
    "    \n",
    "    #Optimizer\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "    \n",
    "    #Predictions\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(tf.matmul(tf_valid_dataset, weights) + biases)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 17.400478\n",
      "Minibatch accuracy: 15.6%\n",
      "Validation accuracy: 15.2%\n",
      "Minibatch loss at step 500: 2.794219\n",
      "Minibatch accuracy: 75.8%\n",
      "Validation accuracy: 76.8%\n",
      "Minibatch loss at step 1000: 1.374625\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 78.9%\n",
      "Minibatch loss at step 1500: 1.594421\n",
      "Minibatch accuracy: 76.6%\n",
      "Validation accuracy: 80.0%\n",
      "Minibatch loss at step 2000: 1.064646\n",
      "Minibatch accuracy: 76.6%\n",
      "Validation accuracy: 80.6%\n",
      "Minibatch loss at step 2500: 1.002532\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 81.9%\n",
      "Minibatch loss at step 3000: 0.761428\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 82.2%\n",
      "Test accuracy: 87.9%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta : 10e-4}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_steps = 3001\n",
    "regul_val = [pow(10, i) for i in np.arange(-4, -2, 0.1)]\n",
    "accuracy_val = []\n",
    "\n",
    "for regul in regul_val:\n",
    "  with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "      offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "      batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "      batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "      feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta : regul}\n",
    "      _, l, predictions = session.run(\n",
    "        [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    accuracy_val.append(accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.semilogx(regul_val, accuracy_val)\n",
    "plt.grid(True)\n",
    "plt.title('Test accuracy by regularization (logistic)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we add the regularization to the neural net with ReLU hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_nodes = 1024\n",
    "\n",
    "graph2 = tf.Graph()\n",
    "with graph2.as_default():\n",
    "    \n",
    "    #Input\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size*image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    beta = tf.placeholder(tf.float32)\n",
    "    \n",
    "    #Variables\n",
    "    weights1 = tf.Variable(tf.truncated_normal([image_size*image_size, num_nodes]))\n",
    "    biases1 = tf.Variable(tf.zeros([num_nodes]))\n",
    "    weights2 = tf.Variable(tf.truncated_normal([num_nodes, num_labels]))\n",
    "    biases2 = tf.Variable(tf.zeros([num_labels]))\n",
    "    beta = tf.placeholder(tf.float32)\n",
    "    \n",
    "    #Computation\n",
    "    hidden_layer1 = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "    logits = tf.matmul(hidden_layer1, weights2) + biases2\n",
    "    prbs = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=tf_train_labels)\n",
    "    loss = tf.reduce_mean(prbs + beta*(tf.nn.l2_loss(weights1) + tf.nn.l2_loss(weights2)))\n",
    "    \n",
    "    #Optimizer\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "    \n",
    "    #Predictions\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    layer1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
    "    valid_prediction = tf.nn.softmax(tf.matmul(layer1_valid, weights2) + biases2)\n",
    "    layer1_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(layer1_test, weights2) + biases2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 598.062317\n",
      "Minibatch accuracy: 10.2%\n",
      "Validation accuracy: 26.3%\n",
      "Minibatch loss at step 500: 191.131378\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 82.2%\n",
      "Minibatch loss at step 1000: 114.305740\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 82.4%\n",
      "Minibatch loss at step 1500: 69.577576\n",
      "Minibatch accuracy: 77.3%\n",
      "Validation accuracy: 82.5%\n",
      "Minibatch loss at step 2000: 41.701920\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 84.4%\n",
      "Minibatch loss at step 2500: 25.488457\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 86.4%\n",
      "Minibatch loss at step 3000: 15.424375\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 88.1%\n",
      "Test accuracy: 92.7%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph2) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta : 1e-3}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "na8xX2yHZzNF"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "Let's demonstrate an extreme case of overfitting. Restrict your training data to just a few batches. What happens?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_nodes = 1024\n",
    "\n",
    "graph2 = tf.Graph()\n",
    "with graph2.as_default():\n",
    "    \n",
    "    #Input\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size*image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    beta = tf.placeholder(tf.float32)\n",
    "    \n",
    "    #Variables\n",
    "    weights1 = tf.Variable(tf.truncated_normal([image_size*image_size, num_nodes]))\n",
    "    biases1 = tf.Variable(tf.zeros([num_nodes]))\n",
    "    weights2 = tf.Variable(tf.truncated_normal([num_nodes, num_labels]))\n",
    "    biases2 = tf.Variable(tf.zeros([num_labels]))\n",
    "    beta = tf.placeholder(tf.float32)\n",
    "    \n",
    "    #Computation\n",
    "    hidden_layer1 = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "    logits = tf.matmul(hidden_layer1, weights2) + biases2\n",
    "    prbs = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=tf_train_labels)\n",
    "    loss = tf.reduce_mean(prbs + beta*(tf.nn.l2_loss(weights1) + tf.nn.l2_loss(weights2)))\n",
    "    \n",
    "    #Optimizer\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "    \n",
    "    #Predictions\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    layer1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
    "    valid_prediction = tf.nn.softmax(tf.matmul(layer1_valid, weights2) + biases2)\n",
    "    layer1_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(layer1_test, weights2) + biases2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 708.006836\n",
      "Minibatch accuracy: 14.8%\n",
      "Validation accuracy: 27.7%\n",
      "Minibatch loss at step 500: 191.266937\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 75.6%\n",
      "Minibatch loss at step 1000: 115.994736\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 75.6%\n",
      "Minibatch loss at step 1500: 70.345703\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 75.6%\n",
      "Minibatch loss at step 2000: 42.661934\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 75.6%\n",
      "Minibatch loss at step 2500: 25.873693\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 75.6%\n",
      "Minibatch loss at step 3000: 15.693985\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 75.8%\n",
      "Test accuracy: 82.5%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "num_batches = 5\n",
    "\n",
    "with tf.Session(graph=graph2) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = ((step % num_batches) * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta : 1e-3}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ww3SCBUdlkRc"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "Introduce Dropout on the hidden layer of the neural network. Remember: Dropout should only be introduced during training, not evaluation, otherwise your evaluation results would be stochastic as well. TensorFlow provides `nn.dropout()` for that, but you have to make sure it's only inserted during training.\n",
    "\n",
    "What happens to our extreme overfitting case?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_nodes = 1024\n",
    "\n",
    "graph2 = tf.Graph()\n",
    "with graph2.as_default():\n",
    "    \n",
    "    #Input\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size*image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    beta = tf.placeholder(tf.float32)\n",
    "    \n",
    "    #Variables\n",
    "    weights1 = tf.Variable(tf.truncated_normal([image_size*image_size, num_nodes]))\n",
    "    biases1 = tf.Variable(tf.zeros([num_nodes]))\n",
    "    weights2 = tf.Variable(tf.truncated_normal([num_nodes, num_labels]))\n",
    "    biases2 = tf.Variable(tf.zeros([num_labels]))\n",
    "    beta = tf.placeholder(tf.float32)\n",
    "    \n",
    "    #Computation\n",
    "    hidden_layer1 = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "    drop = tf.nn.dropout(hidden_layer1, 0.5)\n",
    "    logits = tf.matmul(drop, weights2) + biases2\n",
    "    prbs = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=tf_train_labels)\n",
    "    loss = tf.reduce_mean(prbs + beta*(tf.nn.l2_loss(weights1) + tf.nn.l2_loss(weights2)))\n",
    "    \n",
    "    #Optimizer\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "    \n",
    "    #Predictions\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    layer1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
    "    valid_prediction = tf.nn.softmax(tf.matmul(layer1_valid, weights2) + biases2)\n",
    "    layer1_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(layer1_test, weights2) + biases2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 770.085083\n",
      "Minibatch accuracy: 12.5%\n",
      "Validation accuracy: 32.2%\n",
      "Minibatch loss at step 500: 191.822922\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 78.8%\n",
      "Minibatch loss at step 1000: 116.406342\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 78.5%\n",
      "Minibatch loss at step 1500: 70.615097\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 78.9%\n",
      "Minibatch loss at step 2000: 42.903442\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 78.6%\n",
      "Minibatch loss at step 2500: 25.979364\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 78.8%\n",
      "Minibatch loss at step 3000: 15.756669\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 78.9%\n",
      "Test accuracy: 86.5%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "num_batches = 5\n",
    "\n",
    "with tf.Session(graph=graph2) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = ((step % num_batches) * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta : 1e-3}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-b1hTz3VWZjw"
   },
   "source": [
    "---\n",
    "Problem 4\n",
    "---------\n",
    "\n",
    "Try to get the best performance you can using a multi-layer model! The best reported test accuracy using a deep network is [97.1%](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html?showComment=1391023266211#c8758720086795711595).\n",
    "\n",
    "One avenue you can explore is to add multiple layers.\n",
    "\n",
    "Another one is to use learning rate decay:\n",
    "\n",
    "    global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "    learning_rate = tf.train.exponential_decay(0.5, global_step, ...)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    " \n",
    " ---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "num_nodes1 = 1024\n",
    "num_nodes2 = 512\n",
    "num_nodes3 = 60\n",
    "\n",
    "graph3 = tf.Graph()\n",
    "with graph3.as_default():\n",
    "    \n",
    "    #Input\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size*image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    beta = tf.placeholder(tf.float32)\n",
    "    \n",
    "    #Variables\n",
    "    weights1 = tf.Variable(tf.truncated_normal([image_size*image_size, num_nodes1],stddev=np.sqrt(2.0 / (image_size * image_size))))\n",
    "    biases1 = tf.Variable(tf.zeros([num_nodes1]))\n",
    "    weights2 = tf.Variable(tf.truncated_normal([num_nodes1, num_nodes2],stddev=np.sqrt(2.0 /num_nodes1)))\n",
    "    biases2 = tf.Variable(tf.zeros([num_nodes2]))\n",
    "    weights3 = tf.Variable(tf.truncated_normal([num_nodes2, num_nodes3],stddev=np.sqrt(2.0 /num_nodes2)))\n",
    "    biases3 = tf.Variable(tf.zeros([num_nodes3]))\n",
    "    weights4 = tf.Variable(tf.truncated_normal([num_nodes3, num_labels],stddev=np.sqrt(2.0 /num_nodes3)))\n",
    "    biases4 = tf.Variable(tf.zeros([num_labels]))\n",
    "    \n",
    "    global_step = tf.Variable(0)\n",
    "    beta = tf.placeholder(tf.float32)\n",
    "    \n",
    "    #Computation\n",
    "    hidden_layer1 = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "    drop1 = tf.nn.dropout(hidden_layer1, 0.5)\n",
    "    hidden_layer2 = tf.nn.relu(tf.matmul(drop1, weights2) + biases2)\n",
    "    drop2 = tf.nn.dropout(hidden_layer2, 0.5)\n",
    "    hidden_layer3 = tf.nn.relu(tf.matmul(drop2, weights3) + biases3)\n",
    "    drop3 = tf.nn.dropout(hidden_layer3, 0.5)\n",
    "    logits = tf.matmul(drop3, weights4) + biases4\n",
    "    prbs = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=tf_train_labels)\n",
    "    loss = tf.reduce_mean(prbs) #+ beta*(tf.nn.l2_loss(weights1) + tf.nn.l2_loss(weights2) + tf.nn.l2_loss(weights3)))\n",
    "    \n",
    "    #Optimizer\n",
    "    learning_rate = tf.train.exponential_decay(0.5, global_step, 1000, 0.9)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "    \n",
    "    #Predictions\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    layer1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
    "    layer2_valid = tf.nn.relu(tf.matmul(layer1_valid, weights2) + biases2)\n",
    "    layer3_valid = tf.nn.relu(tf.matmul(layer2_valid, weights3) + biases3)\n",
    "    valid_prediction = tf.nn.softmax(tf.matmul(layer3_valid, weights4) + biases4)\n",
    "    layer1_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "    layer2_test = tf.nn.relu(tf.matmul(layer1_test, weights2) + biases2)\n",
    "    layer3_test = tf.nn.relu(tf.matmul(layer2_test, weights3) + biases3)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(layer3_test, weights4) + biases4)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 2.673212\n",
      "Minibatch accuracy: 9.8%\n",
      "Validation accuracy: 26.5%\n",
      "Minibatch loss at step 500: 0.642299\n",
      "Minibatch accuracy: 81.6%\n",
      "Validation accuracy: 86.0%\n",
      "Minibatch loss at step 1000: 0.468276\n",
      "Minibatch accuracy: 82.4%\n",
      "Validation accuracy: 87.2%\n",
      "Minibatch loss at step 1500: 0.495179\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 87.8%\n",
      "Minibatch loss at step 2000: 0.336768\n",
      "Minibatch accuracy: 90.2%\n",
      "Validation accuracy: 88.4%\n",
      "Minibatch loss at step 2500: 0.417498\n",
      "Minibatch accuracy: 88.7%\n",
      "Validation accuracy: 89.1%\n",
      "Minibatch loss at step 3000: 0.464351\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 88.9%\n",
      "Minibatch loss at step 3500: 0.421913\n",
      "Minibatch accuracy: 86.3%\n",
      "Validation accuracy: 89.2%\n",
      "Minibatch loss at step 4000: 0.445144\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 89.5%\n",
      "Minibatch loss at step 4500: 0.279720\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 89.8%\n",
      "Minibatch loss at step 5000: 0.389723\n",
      "Minibatch accuracy: 87.1%\n",
      "Validation accuracy: 89.6%\n",
      "Minibatch loss at step 5500: 0.376554\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 90.2%\n",
      "Minibatch loss at step 6000: 0.335284\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 90.0%\n",
      "Minibatch loss at step 6500: 0.333395\n",
      "Minibatch accuracy: 89.5%\n",
      "Validation accuracy: 90.3%\n",
      "Minibatch loss at step 7000: 0.303713\n",
      "Minibatch accuracy: 91.0%\n",
      "Validation accuracy: 90.0%\n",
      "Minibatch loss at step 7500: 0.429482\n",
      "Minibatch accuracy: 87.1%\n",
      "Validation accuracy: 90.2%\n",
      "Minibatch loss at step 8000: 0.420217\n",
      "Minibatch accuracy: 87.9%\n",
      "Validation accuracy: 90.6%\n",
      "Minibatch loss at step 8500: 0.316034\n",
      "Minibatch accuracy: 89.5%\n",
      "Validation accuracy: 90.5%\n",
      "Minibatch loss at step 9000: 0.345909\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 90.4%\n",
      "Minibatch loss at step 9500: 0.249498\n",
      "Minibatch accuracy: 93.4%\n",
      "Validation accuracy: 90.7%\n",
      "Minibatch loss at step 10000: 0.305407\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 90.6%\n",
      "Minibatch loss at step 10500: 0.267062\n",
      "Minibatch accuracy: 91.8%\n",
      "Validation accuracy: 90.7%\n",
      "Minibatch loss at step 11000: 0.233847\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 90.8%\n",
      "Minibatch loss at step 11500: 0.319157\n",
      "Minibatch accuracy: 89.5%\n",
      "Validation accuracy: 90.7%\n",
      "Minibatch loss at step 12000: 0.316057\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 91.1%\n",
      "Minibatch loss at step 12500: 0.376710\n",
      "Minibatch accuracy: 87.1%\n",
      "Validation accuracy: 90.9%\n",
      "Minibatch loss at step 13000: 0.456844\n",
      "Minibatch accuracy: 84.8%\n",
      "Validation accuracy: 91.1%\n",
      "Minibatch loss at step 13500: 0.255121\n",
      "Minibatch accuracy: 91.8%\n",
      "Validation accuracy: 90.8%\n",
      "Minibatch loss at step 14000: 0.206902\n",
      "Minibatch accuracy: 93.4%\n",
      "Validation accuracy: 91.0%\n",
      "Minibatch loss at step 14500: 0.322942\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 91.1%\n",
      "Minibatch loss at step 15000: 0.279648\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 91.3%\n",
      "Test accuracy: 95.2%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 15001\n",
    "\n",
    "with tf.Session(graph=graph3) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}#, beta : 1e-3}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test accuracy is __95.2__ percent! This is amazing!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "3_regularization.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
